{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import tensorflow.compat.v1 as tf\n",
    "tf.disable_v2_behavior() \n",
    "import numpy as np\n",
    "import numpy\n",
    "import math\n",
    "import timeit\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras import backend as k\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras import Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "USE_GPU = False\n",
    "\n",
    "if USE_GPU:\n",
    "    device = '/device:GPU:0'\n",
    "else:\n",
    "    device = '/cpu:0'\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"temp_config\"\"\"\n",
    "\n",
    "\n",
    "batch_size = 1\n",
    "\n",
    "\n",
    "k = 16\n",
    "k2 = 8\n",
    "output_size = 7\n",
    "output_size_valuehead = 10\n",
    "#batch_size = 64\n",
    "input_dims = [batch_size, 6, 7, 8]\n",
    "new_dims = [batch_size, 6 * 7 * 8]\n",
    "input_prodsize = 8 * 6 * 7\n",
    "after_input_dims = [batch_size, 6, 7, 8]\n",
    "after_new_dims = [batch_size, 6 * 7 * 8]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "def two_layer_fc_functional(inputs, hidden_size, num_classes):     \n",
    "    initializer = tf.variance_scaling_initializer(scale=2.0)\n",
    "    flattened_inputs = tf.layers.flatten(inputs)\n",
    "    fc1_output = tf.layers.dense(flattened_inputs, hidden_size, activation=tf.nn.relu,\n",
    "                                 kernel_initializer=initializer)\n",
    "    scores = tf.layers.dense(fc1_output, num_classes,\n",
    "                             kernel_initializer=initializer)\n",
    "    return scores\n",
    "\n",
    "\n",
    "\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_two_layer_fc_functional():\n",
    "    \"\"\" A small unit test to exercise the TwoLayerFC model above. \"\"\"\n",
    "    tf.reset_default_graph()\n",
    "    input_size, hidden_size, num_classes = 50, 42, 10\n",
    "\n",
    "    # As usual in TensorFlow, we first need to define our computational graph.\n",
    "    # To this end we first construct a two layer network graph by calling the\n",
    "    # two_layer_network() function. This function constructs the computation\n",
    "    # graph and outputs the score tensor.\n",
    "    with tf.device(device):\n",
    "        x = tf.zeros((64, input_size))\n",
    "        scores = two_layer_fc_functional(x, hidden_size, num_classes)\n",
    "\n",
    "    # Now that our computational graph has been defined we can run the graph\n",
    "    with tf.Session() as sess:\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "        scores_np = sess.run(scores)\n",
    "        print(scores_np.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "#test_two_layer_fc_functional()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### New Model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvNet(tf.keras.Model):\n",
    "    def __init__(self, channel_1):\n",
    "        super().__init__()\n",
    "        initializer = tf.variance_scaling_initializer(scale=2.0) \n",
    "          \n",
    "        self.conv1 = tf.layers.Conv2D(channel_1, (3,3), (1,1), padding='same',\n",
    "                                      kernel_initializer=initializer,\n",
    "                                      activation=None)\n",
    "        \n",
    "    def call(self, x, training=None):\n",
    "        scores = None\n",
    "        \n",
    "        scores = self.conv1(x)\n",
    "            \n",
    "        return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "def DENSE_block(inputs, num_units, kernel_initializer, activation, input_dims, new_dims):\n",
    "    l_re_1 = tf.reshape(inputs, new_dims)\n",
    "    #might want to add initializer\n",
    "    l_re_2 = tf.layers.dense(l_re_1, num_units, activation)\n",
    "    return l_re_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ADD_bias_block(inputs, train_bool, input_dims):\n",
    "    \n",
    "    \n",
    "    \n",
    "    l_hid_re = tf.reshape(inputs, new_dims)\n",
    "    l_hid_1 = tf.layers.dense(inputs, 1, activation = None)\n",
    "    l_hid_2 = tf.math.scalar_mul(0, l_hid_1)\n",
    "    l_hid_3 = tf.layers.dense(l_hid_re, input_prodsize, activation = None)\n",
    "    l_hid_4 = tf.reshape(l_hid_3, input_dims)\n",
    "    return l_hid_4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "def BASE_block(inputs, train_bool, initializer):\n",
    "    #l_hidden_1 = tf.layers.conv2d(inputs, k, (3,3), (1,1), 'same', kernel_initializer=initializer, activation = None)\n",
    "    l_hidden_1 = tf.layers.conv2d(inputs, k, (3,3), (1,1), 'same', kernel_initializer=initializer, activation = None)\n",
    "    #l_hidden_2 = tf.layers.batch_normalization(l_hidden_1, training = train_bool)\n",
    "    l_hidden_3 = tf.nn.relu(l_hidden_1)\n",
    "    return l_hidden_3\n",
    "\n",
    "        \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "def RESIDUAL_block(inputs, train_bool, initializer):\n",
    "    l_resid_1 = tf.layers.conv2d(inputs, k, (3,3), (1,1), 'same', kernel_initializer=initializer, activation = None)\n",
    "    #l_resid_2 = tf.layers.batch_normalization(l_resid_1, training = train_bool)\n",
    "    l_resid_3 = tf.nn.relu(l_resid_1)\n",
    "    l_resid_4 = tf.layers.conv2d(l_resid_3, k, (3,3), (1,1), 'same', kernel_initializer=initializer, activation = None)\n",
    "    #l_resid_5 = tf.layers.batch_normalization(l_resid_4, training = train_bool)\n",
    "    l_resid_6 = tf.keras.layers.Add()([l_resid_1, l_resid_4])\n",
    "    l_resid_7 = tf.nn.relu(l_resid_6)\n",
    "    return l_resid_7\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "def RESIDUAL_tower(inputs, train_bool, initializer):\n",
    "    l_resid_1 = RESIDUAL_block(inputs, train_bool, initializer)\n",
    "    l_resid_2 = RESIDUAL_block(l_resid_1, train_bool, initializer)\n",
    "    l_resid_3 = RESIDUAL_block(l_resid_2, train_bool, initializer)\n",
    "    l_resid_4 = RESIDUAL_block(l_resid_3, train_bool, initializer)\n",
    "    l_resid_5 = RESIDUAL_block(l_resid_4, train_bool, initializer)\n",
    "    l_resid_6 = RESIDUAL_block(l_resid_5, train_bool, initializer)\n",
    "    return l_resid_6\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"def ACTION_head(inputs, train_bool, initializer):\n",
    "    l_action_1 = tf.layers.conv2d(inputs, k2, (1,1), (1,1), 'same', kernel_initializer=initializer, activation = None)\n",
    "    l_action_2 = ADD_bias_block(l_action_1, train_bool, input_dims)\n",
    "    l_action_3 = tf.layers.batch_normalization(l_action_2, training = train_bool)\n",
    "    l_action_4 = tf.nn.relu(l_action_3)\n",
    "    l_action_5 = DENSE_block(l_action_4, 7, initializer, tf.nn.softmax, after_input_dims, after_new_dims)\n",
    "    return l_action_5\"\"\"\n",
    "    \n",
    "def ACTION_head(inputs, train_bool, initializer):\n",
    "    l_action_1 = tf.layers.conv2d(inputs, k2, (1,1), (1,1), 'same', kernel_initializer=initializer, activation = None)\n",
    "    #l_action_3 = tf.layers.batch_normalization(l_action_1, training = train_bool)\n",
    "    l_action_4 = tf.nn.relu(l_action_1)\n",
    "    l_action_5 = DENSE_block(l_action_4, 7, initializer, tf.nn.softmax, after_input_dims, after_new_dims)\n",
    "    return l_action_5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"def VALUE_head(inputs, train_bool, initializer):\n",
    "    l_value_1 = tf.layers.conv2d(inputs, k2, (1,1), (1,1), 'same', kernel_initializer=initializer, activation = None)\n",
    "    l_value_2 = ADD_bias_block(l_value_1, train_bool, input_dims)\n",
    "    l_value_3 = tf.layers.batch_normalization(l_value_2, training = train_bool)\n",
    "    l_value_4 = tf.nn.relu(l_value_3)\n",
    "    l_value_5 = DENSE_block(l_action_4, 10, initializer, tf.nn.softmax, after_input_dims, after_new_dims)\n",
    "    l_value_6 = tf.layers.dense(l_value_5, 1, nonlinearity = tf.nn.sigmoid)\n",
    "    return l_value_6\"\"\"\n",
    "    \n",
    "def VALUE_head(inputs, train_bool, initializer):\n",
    "    l_value_1 = tf.layers.conv2d(inputs, k2, (1,1), (1,1), 'same', kernel_initializer=initializer, activation = None)\n",
    "    #l_value_3 = tf.layers.batch_normalization(l_value_1, training = train_bool)\n",
    "    l_value_4 = tf.nn.relu(l_value_1)\n",
    "    l_value_5 = DENSE_block(l_action_4, 10, initializer, tf.nn.softmax, after_input_dims, after_new_dims)\n",
    "    l_value_6 = tf.layers.dense(l_value_5, 1, nonlinearity = tf.nn.sigmoid)\n",
    "    return l_value_6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "def RESIDUAL_net_action_1(inputs, train_bool):\n",
    "    #might change this\n",
    "    initializer = tf.variance_scaling_initializer(scale=2.0)\n",
    "    l_block_1 = BASE_block(inputs, train_bool, initializer)\n",
    "    l_block_2 = RESIDUAL_tower(l_block_1, train_bool, initializer)\n",
    "    l_block_3 = ACTION_head(l_block_2, train_bool, initializer)\n",
    "    return l_block_3\n",
    "\n",
    "\n",
    "def RESIDUAL_net_action_2(inputs, train_bool):\n",
    "    #might change this\n",
    "    initializer = tf.variance_scaling_initializer(scale=2.0)\n",
    "    l_block_1 = BASE_block(inputs, train_bool, initializer)\n",
    "    l_block_2 = RESIDUAL_tower(l_block_1, train_bool, initializer)\n",
    "    l_block_3 = ACTION_head(l_block_2, train_bool, initializer)\n",
    "    return l_block_3\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "def RESIDUAL_net_value_1(inputs, train_bool):\n",
    "    #might change this\n",
    "    initializer = tf.variance_scaling_initializer(scale=2.0)\n",
    "    l_block_1 = BASE_block(inputs, train_bool, initializer)\n",
    "    l_block_2 = RESIDUAL_tower(l_block_1, train_bool, initializer)\n",
    "    l_block_3 = VALUE_head(l_block_2, train_bool, initializer)\n",
    "    return l_block_3\n",
    "\n",
    "\n",
    "def RESIDUAL_net_value_2(inputs, train_bool):\n",
    "    #might change this\n",
    "    initializer = tf.variance_scaling_initializer(scale=2.0)\n",
    "    l_block_1 = BASE_block(inputs, train_bool, initializer)\n",
    "    l_block_2 = RESIDUAL_tower(l_block_1, train_bool, initializer)\n",
    "    l_block_3 = VALUE_head(l_block_2, train_bool, initializer)\n",
    "    return l_block_3\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "#label = [1] * batch_size\n",
    "def test_RESIDUAL_net_action_1(label):\n",
    "    \n",
    "    tf.reset_default_graph()\n",
    "    \n",
    "    model = RESIDUAL_net_action_1\n",
    "    with tf.device(device):\n",
    "        x = tf.zeros((1, 6, 7, 2), dtype=tf.dtypes.float32)\n",
    "        #x = np.array([[[[0]*7]*6]*2]*1)\n",
    "        scores = model(x, True)\n",
    "\n",
    "    # Now that our computational graph has been defined we can run the graph\n",
    "    with tf.Session() as sess:\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "        scores_np = sess.run(scores)\n",
    "        \n",
    "        \n",
    "        \n",
    "        loss = tf.nn.sparse_softmax_cross_entropy_with_logits(labels = label, logits=scores)\n",
    "        \n",
    "        loss = tf.reduce_mean(loss)\n",
    "        \n",
    "        \n",
    "        \n",
    "        varis = tf.trainable_variables()\n",
    "        params = varis\n",
    "        \n",
    "        \n",
    "        grad_params = tf.gradients(loss, params)\n",
    "        \n",
    "        \n",
    "        #print(scores_np.shape)\n",
    "        #print(grad_params)\n",
    "        return (params, grad_params)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([<tf.Variable 'conv2d/kernel:0' shape=(3, 3, 2, 16) dtype=float32_ref>,\n",
       "  <tf.Variable 'conv2d/bias:0' shape=(16,) dtype=float32_ref>,\n",
       "  <tf.Variable 'conv2d_1/kernel:0' shape=(3, 3, 16, 16) dtype=float32_ref>,\n",
       "  <tf.Variable 'conv2d_1/bias:0' shape=(16,) dtype=float32_ref>,\n",
       "  <tf.Variable 'conv2d_2/kernel:0' shape=(3, 3, 16, 16) dtype=float32_ref>,\n",
       "  <tf.Variable 'conv2d_2/bias:0' shape=(16,) dtype=float32_ref>,\n",
       "  <tf.Variable 'conv2d_3/kernel:0' shape=(3, 3, 16, 16) dtype=float32_ref>,\n",
       "  <tf.Variable 'conv2d_3/bias:0' shape=(16,) dtype=float32_ref>,\n",
       "  <tf.Variable 'conv2d_4/kernel:0' shape=(3, 3, 16, 16) dtype=float32_ref>,\n",
       "  <tf.Variable 'conv2d_4/bias:0' shape=(16,) dtype=float32_ref>,\n",
       "  <tf.Variable 'conv2d_5/kernel:0' shape=(3, 3, 16, 16) dtype=float32_ref>,\n",
       "  <tf.Variable 'conv2d_5/bias:0' shape=(16,) dtype=float32_ref>,\n",
       "  <tf.Variable 'conv2d_6/kernel:0' shape=(3, 3, 16, 16) dtype=float32_ref>,\n",
       "  <tf.Variable 'conv2d_6/bias:0' shape=(16,) dtype=float32_ref>,\n",
       "  <tf.Variable 'conv2d_7/kernel:0' shape=(3, 3, 16, 16) dtype=float32_ref>,\n",
       "  <tf.Variable 'conv2d_7/bias:0' shape=(16,) dtype=float32_ref>,\n",
       "  <tf.Variable 'conv2d_8/kernel:0' shape=(3, 3, 16, 16) dtype=float32_ref>,\n",
       "  <tf.Variable 'conv2d_8/bias:0' shape=(16,) dtype=float32_ref>,\n",
       "  <tf.Variable 'conv2d_9/kernel:0' shape=(3, 3, 16, 16) dtype=float32_ref>,\n",
       "  <tf.Variable 'conv2d_9/bias:0' shape=(16,) dtype=float32_ref>,\n",
       "  <tf.Variable 'conv2d_10/kernel:0' shape=(3, 3, 16, 16) dtype=float32_ref>,\n",
       "  <tf.Variable 'conv2d_10/bias:0' shape=(16,) dtype=float32_ref>,\n",
       "  <tf.Variable 'conv2d_11/kernel:0' shape=(3, 3, 16, 16) dtype=float32_ref>,\n",
       "  <tf.Variable 'conv2d_11/bias:0' shape=(16,) dtype=float32_ref>,\n",
       "  <tf.Variable 'conv2d_12/kernel:0' shape=(3, 3, 16, 16) dtype=float32_ref>,\n",
       "  <tf.Variable 'conv2d_12/bias:0' shape=(16,) dtype=float32_ref>,\n",
       "  <tf.Variable 'conv2d_13/kernel:0' shape=(1, 1, 16, 8) dtype=float32_ref>,\n",
       "  <tf.Variable 'conv2d_13/bias:0' shape=(8,) dtype=float32_ref>,\n",
       "  <tf.Variable 'dense/kernel:0' shape=(336, 7) dtype=float32_ref>,\n",
       "  <tf.Variable 'dense/bias:0' shape=(7,) dtype=float32_ref>],\n",
       " [<tf.Tensor 'gradients/conv2d/Conv2D_grad/Conv2DBackpropFilter:0' shape=(3, 3, 2, 16) dtype=float32>,\n",
       "  <tf.Tensor 'gradients/conv2d/BiasAdd_grad/BiasAddGrad:0' shape=(16,) dtype=float32>,\n",
       "  <tf.Tensor 'gradients/conv2d_1/Conv2D_grad/Conv2DBackpropFilter:0' shape=(3, 3, 16, 16) dtype=float32>,\n",
       "  <tf.Tensor 'gradients/conv2d_1/BiasAdd_grad/BiasAddGrad:0' shape=(16,) dtype=float32>,\n",
       "  <tf.Tensor 'gradients/conv2d_2/Conv2D_grad/Conv2DBackpropFilter:0' shape=(3, 3, 16, 16) dtype=float32>,\n",
       "  <tf.Tensor 'gradients/conv2d_2/BiasAdd_grad/BiasAddGrad:0' shape=(16,) dtype=float32>,\n",
       "  <tf.Tensor 'gradients/conv2d_3/Conv2D_grad/Conv2DBackpropFilter:0' shape=(3, 3, 16, 16) dtype=float32>,\n",
       "  <tf.Tensor 'gradients/conv2d_3/BiasAdd_grad/BiasAddGrad:0' shape=(16,) dtype=float32>,\n",
       "  <tf.Tensor 'gradients/conv2d_4/Conv2D_grad/Conv2DBackpropFilter:0' shape=(3, 3, 16, 16) dtype=float32>,\n",
       "  <tf.Tensor 'gradients/conv2d_4/BiasAdd_grad/BiasAddGrad:0' shape=(16,) dtype=float32>,\n",
       "  <tf.Tensor 'gradients/conv2d_5/Conv2D_grad/Conv2DBackpropFilter:0' shape=(3, 3, 16, 16) dtype=float32>,\n",
       "  <tf.Tensor 'gradients/conv2d_5/BiasAdd_grad/BiasAddGrad:0' shape=(16,) dtype=float32>,\n",
       "  <tf.Tensor 'gradients/conv2d_6/Conv2D_grad/Conv2DBackpropFilter:0' shape=(3, 3, 16, 16) dtype=float32>,\n",
       "  <tf.Tensor 'gradients/conv2d_6/BiasAdd_grad/BiasAddGrad:0' shape=(16,) dtype=float32>,\n",
       "  <tf.Tensor 'gradients/conv2d_7/Conv2D_grad/Conv2DBackpropFilter:0' shape=(3, 3, 16, 16) dtype=float32>,\n",
       "  <tf.Tensor 'gradients/conv2d_7/BiasAdd_grad/BiasAddGrad:0' shape=(16,) dtype=float32>,\n",
       "  <tf.Tensor 'gradients/conv2d_8/Conv2D_grad/Conv2DBackpropFilter:0' shape=(3, 3, 16, 16) dtype=float32>,\n",
       "  <tf.Tensor 'gradients/conv2d_8/BiasAdd_grad/BiasAddGrad:0' shape=(16,) dtype=float32>,\n",
       "  <tf.Tensor 'gradients/conv2d_9/Conv2D_grad/Conv2DBackpropFilter:0' shape=(3, 3, 16, 16) dtype=float32>,\n",
       "  <tf.Tensor 'gradients/conv2d_9/BiasAdd_grad/BiasAddGrad:0' shape=(16,) dtype=float32>,\n",
       "  <tf.Tensor 'gradients/conv2d_10/Conv2D_grad/Conv2DBackpropFilter:0' shape=(3, 3, 16, 16) dtype=float32>,\n",
       "  <tf.Tensor 'gradients/conv2d_10/BiasAdd_grad/BiasAddGrad:0' shape=(16,) dtype=float32>,\n",
       "  <tf.Tensor 'gradients/conv2d_11/Conv2D_grad/Conv2DBackpropFilter:0' shape=(3, 3, 16, 16) dtype=float32>,\n",
       "  <tf.Tensor 'gradients/conv2d_11/BiasAdd_grad/BiasAddGrad:0' shape=(16,) dtype=float32>,\n",
       "  <tf.Tensor 'gradients/conv2d_12/Conv2D_grad/Conv2DBackpropFilter:0' shape=(3, 3, 16, 16) dtype=float32>,\n",
       "  <tf.Tensor 'gradients/conv2d_12/BiasAdd_grad/BiasAddGrad:0' shape=(16,) dtype=float32>,\n",
       "  <tf.Tensor 'gradients/conv2d_13/Conv2D_grad/Conv2DBackpropFilter:0' shape=(1, 1, 16, 8) dtype=float32>,\n",
       "  <tf.Tensor 'gradients/conv2d_13/BiasAdd_grad/BiasAddGrad:0' shape=(8,) dtype=float32>,\n",
       "  <tf.Tensor 'gradients/dense/MatMul_grad/MatMul_1:0' shape=(336, 7) dtype=float32>,\n",
       "  <tf.Tensor 'gradients/dense/BiasAdd_grad/BiasAddGrad:0' shape=(7,) dtype=float32>])"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_RESIDUAL_net_action_1([1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Grad Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#label = [1] * 1\n",
    "\n",
    "def loop_model(x_np, y_np, train_bool, model, params1_v):\n",
    "    \n",
    "    tf.reset_default_graph()\n",
    "    pipe = test_RESIDUAL_net_action_1([1])\n",
    "    loop_model_params_set(train_bool, model, params1_v)\n",
    "    \n",
    "    \n",
    "  \n",
    "    #model = RESIDUAL_net_action_1\n",
    "    \n",
    "    with tf.device(device):\n",
    "        \n",
    "        #change shapes of placeholders\n",
    "        x = tf.placeholder(tf.float32, [None, 6, 7, 2])\n",
    "        y = tf.placeholder(tf.int32, [None])\n",
    "        is_training = tf.placeholder(tf.bool, name='is_training')\n",
    "        \n",
    "        \n",
    "        scores = model(x, is_training)\n",
    "        \n",
    "        loss = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=y, logits=scores)\n",
    "        loss = tf.reduce_mean(loss)\n",
    "        \n",
    "        varis = tf.trainable_variables()\n",
    "        params = varis\n",
    "        \n",
    "        \n",
    "        grad_params = tf.gradients(loss, params)\n",
    "        \n",
    "        \n",
    "        \n",
    "    \n",
    "    # Now that our computational graph has been defined we can run the graph\n",
    "    with tf.Session() as sess:\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "        \n",
    "        feed_dict = {x: x_np, y: y_np, is_training: train_bool}\n",
    "        loss_np, params_np, grad_params_np = sess.run([loss, params, grad_params], feed_dict=feed_dict)\n",
    "\n",
    "        return (loss_np, params_np, grad_params_np)\n",
    "    \n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def loop_model_loss(x_np, y_np, train_bool, model, params1_v):\n",
    "    \n",
    "    \n",
    "    \n",
    "    tf.reset_default_graph()\n",
    "    pipe = test_RESIDUAL_net_action_1([1])\n",
    "    loop_model_params_set(train_bool, model, params1_v)\n",
    "    \n",
    "    \n",
    "    \n",
    "   \n",
    "    \n",
    "    \n",
    "    #model = RESIDUAL_net_action_1\n",
    "    \n",
    "    with tf.device(device):\n",
    "        \n",
    "        #change shapes of placeholders\n",
    "        x = tf.placeholder(tf.float32, [None, 6, 7, 2])\n",
    "        y = tf.placeholder(tf.int32, [None])\n",
    "        is_training = tf.placeholder(tf.bool, name='is_training')\n",
    "        \n",
    "        \n",
    "        scores = model(x, is_training)\n",
    "        \n",
    "        loss = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=y, logits=scores)\n",
    "        loss = tf.reduce_mean(loss)\n",
    "        \n",
    "        \n",
    "        varis = tf.trainable_variables()\n",
    "        params = varis\n",
    "        \n",
    "        \n",
    "        grad_params = tf.gradients(loss, params)\n",
    "        \n",
    "        \n",
    "    \n",
    "    # Now that our computational graph has been defined we can run the graph\n",
    "    with tf.Session() as sess:\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "        \n",
    "        feed_dict = {x: x_np, y: y_np, is_training: train_bool}\n",
    "        loss_np, params_np = sess.run([loss, params], feed_dict=feed_dict)\n",
    "\n",
    "        return (loss_np)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def loop_model_grad(x_np, y_np, train_bool, model, params1_v):\n",
    "    \n",
    "    \n",
    "    tf.reset_default_graph()\n",
    "    \n",
    "    \n",
    "    loop_model_params_set(train_bool, model, params1_v)\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    #model = RESIDUAL_net_action_1\n",
    "    \n",
    "    with tf.device(device):\n",
    "        \n",
    "        #change shapes of placeholders\n",
    "        x = tf.placeholder(tf.float32, [None, 6, 7, 2])\n",
    "        y = tf.placeholder(tf.int32, [None])\n",
    "        is_training = tf.placeholder(tf.bool, name='is_training')\n",
    "        \n",
    "        \n",
    "        scores = model(x, is_training)\n",
    "        \n",
    "        loss = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=y, logits=scores)\n",
    "        loss = tf.reduce_mean(loss)\n",
    "        \n",
    "        varis = tf.trainable_variables()\n",
    "        params = varis\n",
    "        \n",
    "        \n",
    "        grad_params = tf.gradients(loss, params)\n",
    "        #print(grad_params)\n",
    "        \n",
    "        \n",
    "    \n",
    "    # Now that our computational graph has been defined we can run the graph\n",
    "    with tf.Session() as sess:\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "        \n",
    "        feed_dict = {x: x_np, y: y_np, is_training: train_bool}\n",
    "        loss_np, params_np, grad_params_np = sess.run([loss, params, grad_params], feed_dict=feed_dict)\n",
    "\n",
    "        return grad_params_np\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def loop_model_scores(x_np, train_bool, model, params1_v):\n",
    "    \n",
    "    tf.reset_default_graph()\n",
    "    pipe = test_RESIDUAL_net_action_1([1])\n",
    "    loop_model_params_set(train_bool, model, params1_v)\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    #model = RESIDUAL_net_action_1\n",
    "    \n",
    "    with tf.device(device):\n",
    "        \n",
    "        #change shapes of placeholders\n",
    "        x = tf.placeholder(tf.float32, [None, 6, 7, 2])\n",
    "        is_training = tf.placeholder(tf.bool, name='is_training')\n",
    "        \n",
    "        \n",
    "        scores = model(x, is_training)\n",
    "       \n",
    "        \n",
    "        \n",
    "    \n",
    "    # Now that our computational graph has been defined we can run the graph\n",
    "    with tf.Session() as sess:\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "        \n",
    "        feed_dict = {x: x_np, is_training: train_bool}\n",
    "        scores_np = sess.run(scores, feed_dict=feed_dict)\n",
    "\n",
    "        return (scores_np)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#label = [1] * 64\n",
    "\n",
    "def loop_model_score_grad(x_np, y_np, train_bool, model, params1_v):\n",
    "    \n",
    "    \n",
    "    tf.reset_default_graph()\n",
    "    pipe = test_RESIDUAL_net_action_1([1])\n",
    "    loop_model_params_set(train_bool, model, params1_v)\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    #model = RESIDUAL_net_action_1\n",
    "    \n",
    "    with tf.device(device):\n",
    "        \n",
    "        #change shapes of placeholders\n",
    "        x = tf.placeholder(tf.float32, [None, 6, 7, 2])\n",
    "        y = tf.placeholder(tf.int32, [None])\n",
    "        is_training = tf.placeholder(tf.bool, name='is_training')\n",
    "        \n",
    "        \n",
    "        scores = model(x, is_training)\n",
    "        \n",
    "        varis = tf.trainable_variables()\n",
    "        params = varis\n",
    "        \n",
    "        grad_params = tf.gradients(scores, params)\n",
    "        \n",
    "        \n",
    "        \n",
    "    \n",
    "    # Now that our computational graph has been defined we can run the graph\n",
    "    with tf.Session() as sess:\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "        \n",
    "        feed_dict = {x: x_np, y: y_np, is_training: train_bool}\n",
    "        grad_params_np = sess.run(grad_params, feed_dict=feed_dict)\n",
    "\n",
    "        return grad_params_np\n",
    "    \n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def loop_model_params_get(train_bool, model):\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    with tf.device(device):\n",
    "        \n",
    "        #change shapes of placeholders\n",
    "        is_training = tf.placeholder(tf.bool, name='is_training')\n",
    "        varis = tf.trainable_variables()\n",
    "        params = varis\n",
    "        \n",
    "        \n",
    "        \n",
    "    \n",
    "    # Now that our computational graph has been defined we can run the graph\n",
    "    with tf.Session() as sess:\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "        \n",
    "        feed_dict = {is_training: train_bool}\n",
    "        params_np = sess.run(params, feed_dict=feed_dict)\n",
    "\n",
    "        return params_np\n",
    "    \n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loop_model_params_set(train_bool, model, params_new):\n",
    "    \n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    with tf.device(device):\n",
    "        \n",
    "        #change shapes of placeholders\n",
    "        \n",
    "        \n",
    "        varis = tf.trainable_variables()\n",
    "        params = varis\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "    \n",
    "    # Now that our computational graph has been defined we can run the graph\n",
    "    with tf.Session() as sess:\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "        \n",
    "        \n",
    "        \n",
    "        #params_np = sess.run(params, feed_dict=feed_dict)\n",
    "        for i in range(len(varis)):\n",
    "        #for i in range(len(tf.trainable_weights)):\n",
    "\n",
    "            \n",
    "            layer = varis[i]  # Select the layer\n",
    "            #layer = tf.trainable_weights[i]  # Select the layer\n",
    "\n",
    "            # And modify it explicitly in TensorFlow\n",
    "            sess.run(tf.assign(layer,  params_new[i]))\n",
    "\n",
    "\n",
    "\n",
    "#assign_op = x.assign(1)\n",
    "#sess.run(assign_op)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_part34(model_init_fn, optimizer_init_fn, num_epochs=1):\n",
    "    \"\"\"\n",
    "    Simple training loop for use with models defined using tf.keras. It trains\n",
    "    a model for one epoch on the CIFAR-10 training set and periodically checks\n",
    "    accuracy on the CIFAR-10 validation set.\n",
    "    \n",
    "    Inputs:\n",
    "    - model_init_fn: A function that takes no parameters; when called it\n",
    "      constructs the model we want to train: model = model_init_fn()\n",
    "    - optimizer_init_fn: A function which takes no parameters; when called it\n",
    "      constructs the Optimizer object we will use to optimize the model:\n",
    "      optimizer = optimizer_init_fn()\n",
    "    - num_epochs: The number of epochs to train for\n",
    "    \n",
    "    Returns: Nothing, but prints progress during training\n",
    "    \"\"\"\n",
    "    tf.reset_default_graph()    \n",
    "    with tf.device(device):\n",
    "        # Construct the computational graph we will use to train the model. We\n",
    "        # use the model_init_fn to construct the model, declare placeholders for\n",
    "        # the data and labels\n",
    "        x = tf.placeholder(tf.float32, [None, 32, 32, 3])\n",
    "        y = tf.placeholder(tf.int32, [None])\n",
    "        \n",
    "        # We need a place holder to explicitly specify if the model is in the training\n",
    "        # phase or not. This is because a number of layers behaves differently in\n",
    "        # training and in testing, e.g., dropout and batch normalization.\n",
    "        # We pass this variable to the computation graph through feed_dict as shown below.\n",
    "        is_training = tf.placeholder(tf.bool, name='is_training')\n",
    "        \n",
    "        # Use the model function to build the forward pass.\n",
    "        scores = model_init_fn(x, is_training)\n",
    "\n",
    "        # Compute the loss like we did in Part II\n",
    "        loss = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=y, logits=scores)\n",
    "        loss = tf.reduce_mean(loss)\n",
    "        \n",
    "\n",
    "        # Use the optimizer_fn to construct an Optimizer, then use the optimizer\n",
    "        # to set up the training step. Asking TensorFlow to evaluate the\n",
    "        # train_op returned by optimizer.minimize(loss) will cause us to make a\n",
    "        # single update step using the current minibatch of data.\n",
    "        \n",
    "        # Note that we use tf.control_dependencies to force the model to run\n",
    "        # the tf.GraphKeys.UPDATE_OPS at each training step. tf.GraphKeys.UPDATE_OPS\n",
    "        # holds the operators that update the states of the network.\n",
    "        # For example, the tf.layers.batch_normalization function adds the running mean\n",
    "        # and variance update operators to tf.GraphKeys.UPDATE_OPS.\n",
    "        optimizer = optimizer_init_fn()\n",
    "        update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)\n",
    "        with tf.control_dependencies(update_ops):\n",
    "            train_op = optimizer.minimize(loss)\n",
    "\n",
    "    # Now we can run the computational graph many times to train the model.\n",
    "    # When we call sess.run we ask it to evaluate train_op, which causes the\n",
    "    # model to update.\n",
    "    with tf.Session() as sess:\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "        t = 0\n",
    "        for epoch in range(num_epochs):\n",
    "            print('Starting epoch %d' % epoch)\n",
    "            for x_np, y_np in train_dset:\n",
    "                feed_dict = {x: x_np, y: y_np, is_training:1}\n",
    "                loss_np, _ = sess.run([loss, train_op], feed_dict=feed_dict)\n",
    "                if t % print_every == 0:\n",
    "                    print('Iteration %d, loss = %.4f' % (t, loss_np))\n",
    "                    check_accuracy(sess, val_dset, x, scores, is_training=is_training)\n",
    "                    print()\n",
    "                t += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"hidden_size, num_classes = 4000, 10\n",
    "learning_rate = 1e-2\n",
    "\n",
    "def model_init_fn(inputs, is_training):\n",
    "    return two_layer_fc_functional(inputs, hidden_size, num_classes)\n",
    "\n",
    "def optimizer_init_fn():\n",
    "    return tf.train.GradientDescentOptimizer(learning_rate)\n",
    "\n",
    "train_part34(model_init_fn, optimizer_init_fn)\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
